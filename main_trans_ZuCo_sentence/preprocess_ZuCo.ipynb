{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25f743b-73a1-40d1-a603-65bbccdd9171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /Users/williamhan/opt/anaconda3/lib/python3.9/site-packages (2021.8.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "5602f661-06f4-4823-853e-ba04f59ef419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.load.utils_ZuCo import *\n",
    "import scipy.io as io\n",
    "import mat73\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffa59a73-2902-4341-be2d-cda615f8b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matfiles(task:str, subdir = '/task1- SR/Matlab_files/'):\n",
    "    \"\"\"\n",
    "        Args: Task number (\"task1\", \"task2\", \"task3\") plus sub-directory\n",
    "        Return: 12 matlab files (one per subject) for given task\n",
    "    \"\"\"\n",
    "    path = os.getcwd() + subdir #+ task\n",
    "    files = [os.path.join(path,file) for file in os.listdir(path)[1:]]\n",
    "    assert len(files) == 12, 'each task must contain 12 .mat files'\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5b8d312-9350-42de-abbf-5513ddbc7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "        Transforms ET (and EEG data) to use for further analysis (per test subject)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task:str, level:str, scaling='min-max', fillna='zeros'):\n",
    "        \"\"\"\n",
    "            Args: task (\"task1\", \"task2\", or \"task3\"), data level, scaling technique, how to treat NaNs\n",
    "        \"\"\"\n",
    "        tasks = ['task1', 'task2', 'task3']\n",
    "        if task in tasks:\n",
    "            self.task = task\n",
    "        else:\n",
    "            raise Exception('Task can only be one of \"task1\", \"task2\", or \"task3\"') \n",
    "        levels = ['sentence', 'word']\n",
    "        if level in levels:\n",
    "            self.level = level\n",
    "        else:\n",
    "            raise Exception('Data can only be processed on sentence or word level')\n",
    "        #display raw (absolut) values or normalize data according to specified feature scaling technique\n",
    "        feature_scalings = ['min-max', 'mean-norm', 'standard', 'raw']\n",
    "        if scaling in feature_scalings:\n",
    "            self.scaling = scaling\n",
    "        else:\n",
    "            raise Exception('Features must either be min-max scaled, mean-normalized or standardized')\n",
    "        fillnans = ['zeros', 'mean', 'min']\n",
    "        if fillna in fillnans:\n",
    "            self.fillna = fillna\n",
    "        else:\n",
    "            raise Exception('Missing values should be replaced with zeros, the mean or min per feature')\n",
    "    \n",
    "    def __call__(self, subject:int):\n",
    "        \"\"\"\n",
    "            Args: test subject (0-11)\n",
    "            Return: DataFrame with normalized features (i.e., attributes) on sentence or word level\n",
    "        \"\"\"\n",
    "        # subject should not be a property of data transform object (thus, it's not in the init method), \n",
    "        # since we want to apply the same data transformation to each subject\n",
    "        subjects = list(range(12))\n",
    "        if subject not in subjects:\n",
    "            raise Exception('Access subject data with an integer value between 0 - 11')  \n",
    "        files = get_matfiles(self.task)\n",
    "        data = io.loadmat(files[subject], squeeze_me=True, struct_as_record=False)['sentenceData']\n",
    "        \n",
    "        if self.level == 'sentence':\n",
    "            fields = ['SentLen',  'omissionRate', 'nFixations', 'meanPupilSize', 'GD', 'TRT', \n",
    "                      'FFD', 'SFD', 'GPT']\n",
    "            if self.task == 'task1' and subject == 2:\n",
    "                features = np.zeros((len(data)-101, len(fields)))\n",
    "            elif self.task == 'task2' and (subject == 6 or subject == 11):\n",
    "                features = np.zeros((len(data)-50, len(fields)))\n",
    "            elif self.task == 'task3' and subject == 3:\n",
    "                features = np.zeros((len(data)-47, len(fields)))\n",
    "            elif self.task == 'task3' and subject == 7:\n",
    "                features = np.zeros((len(data)-48, len(fields)))\n",
    "            elif self.task == 'task3' and subject == 11:\n",
    "                features = np.zeros((len(data)-89, len(fields)))\n",
    "            else:\n",
    "                features = np.zeros((len(data), len(fields)))\n",
    "\n",
    "        elif self.level == 'word':\n",
    "            if self.task == 'task1' and subject == 2:\n",
    "                n_words = sum([len(sent.word) for i, sent in enumerate(data[:-1]) if i < 150 or i > 249])\n",
    "            elif self.task == 'task2' and subject == 6:\n",
    "                n_words = sum([len(sent.word) for i, sent in enumerate(data) if i > 49])  \n",
    "            elif self.task == 'task2' and subject == 11:\n",
    "                n_words = sum([len(sent.word) for i, sent in enumerate(data) if i < 50 or i > 99])\n",
    "            elif self.task == 'task3' and subject == 3:\n",
    "                n_words = sum([len(sent.word) for i, sent in enumerate(data) if i < 178 or i > 224])\n",
    "            elif self.task == 'task3' and subject == 7:\n",
    "                n_words = sum([len(sent.word) for i, sent in enumerate(data) if i < 359])\n",
    "            elif self.task == 'task3' and subject == 11:\n",
    "                n_words = sum([len(sent.word) for i, sent in enumerate(data) if i < 270 or (i > 313 and i < 362)])\n",
    "            else:\n",
    "                n_words = sum([len(sent.word) for sent in data])\n",
    "            fields = ['Sent_ID', 'Word_ID', 'Word', 'nFixations', 'meanPupilSize', \n",
    "                      'GD', 'TRT', 'FFD', 'SFD', 'GPT', 'WordLen']\n",
    "            df = pd.DataFrame(index=range(n_words), columns=[fields])\n",
    "            k = 0\n",
    "        \n",
    "        idx = 0\n",
    "        for i, sent in enumerate(data):\n",
    "            if (self.task == 'task1' and subject == 2) and ((i >= 150 and i <= 249) or i == 399):\n",
    "                continue\n",
    "            elif (self.task == 'task2' and subject == 6) and (i <= 49):\n",
    "                continue\n",
    "            elif (self.task == 'task2' and subject == 11) and (i >= 50 and i <= 99):\n",
    "                continue\n",
    "            elif (self.task == 'task3' and subject == 3) and (i >= 178 and i <= 224):\n",
    "                continue\n",
    "            elif (self.task == 'task3' and subject == 7) and (i >= 359):\n",
    "                continue\n",
    "            elif (self.task == 'task3' and subject == 11) and ((i >= 270 and i <= 313) or (i >= 362 and i <= 406)):\n",
    "                continue\n",
    "            else:\n",
    "                nwords_fixated = 0\n",
    "                for j, word in enumerate(sent.word):\n",
    "                    token = re.sub('[^\\w\\s]', '', word.content)\n",
    "                    #lowercase words at the beginning of the sentence only\n",
    "                    token = token.lower() if j == 0 else token \n",
    "                    if self.level == 'sentence':\n",
    "                        word_features = [getattr(word, field) if hasattr(word, field)\\\n",
    "                                         and not isinstance(getattr(word, field), np.ndarray) else\\\n",
    "                                         0 for field in fields[2:]]\n",
    "                        features[idx, 2:] += word_features\n",
    "                        nwords_fixated += 0 if len(set(word_features)) == 1 and next(iter(set(word_features))) == 0 else 1\n",
    "                    elif self.level == 'word':\n",
    "                        df.iloc[k, 0] = str(idx)+'_NR' if self.task=='task1' or self.task=='task2'\\\n",
    "                                        else str(idx)+'_TSR'\n",
    "                        df.iloc[k, 1] = j\n",
    "                        df.iloc[k, 2] = token\n",
    "                        df.iloc[k, 3:-1] = [getattr(word, field) if hasattr(word, field)\\\n",
    "                                            and not isinstance(getattr(word, field), np.ndarray) else\\\n",
    "                                            0 for field in fields[3:-1]]\n",
    "                        df.iloc[k, -1] = len(token)\n",
    "                        k += 1\n",
    "\n",
    "                if self.level == 'sentence':\n",
    "                    features[idx, 0] = len(sent.word)\n",
    "                    features[idx, 1] = sent.omissionRate\n",
    "                    #normalize by number of words for which fixations were reported\n",
    "                    features[idx, 2:] /= nwords_fixated\n",
    "                    \n",
    "                idx += 1\n",
    "\n",
    "        #handle -inf, inf and NaN values\n",
    "        if self.level == 'sentence': \n",
    "            features = self.check_inf(features)\n",
    "            \n",
    "        elif self.level == 'word':\n",
    "            if self.fillna == 'zeros':\n",
    "                df.iloc[:,:].fillna(0, inplace=True)\n",
    "            elif self.fillna == 'min':\n",
    "                for i, field in enumerate(fields):\n",
    "                    df.iloc[:,i].fillna(getattr(df, field).values.min(), inplace=True)\n",
    "            elif self.fillna == 'mean':\n",
    "                for i, field in enumerate(fields):\n",
    "                    df.iloc[:,i].fillna(getattr(df, field).values.mean(), inplace=True)\n",
    "                    \n",
    "            df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, inplace=True)\n",
    "\n",
    "        #normalize data according to feature scaling technique\n",
    "        if self.scaling == 'min-max':\n",
    "            if self.level == 'sentence':\n",
    "                features = np.array([(feat - min(feat))/(max(feat) - min(feat)) for feat in features.T])\n",
    "            elif self.level == 'word':\n",
    "                df.iloc[:, 3:] = [(getattr(df,field).values - getattr(df,field).values.min())/\\\n",
    "                                  (getattr(df,field).values.max() - getattr(df,field).values.min())\\\n",
    "                                  for field in fields[3:]]\n",
    "                \n",
    "        elif self.scaling == 'mean-norm':\n",
    "            if self.level == 'sentence':\n",
    "                features = np.array([(feat - np.mean(feat))/(max(feat) - min(feat)) for feat in features.T])\n",
    "            elif self.level == 'word':\n",
    "                df.iloc[:, 3:] = [(getattr(df,field).values - getattr(df,field).values.mean())/\\\n",
    "                                  (getattr(df,field).values.max() - getattr(df,field).values.min())\\\n",
    "                                  for field in fields[3:]]\n",
    "                \n",
    "        elif self.scaling == 'standard':\n",
    "            if self.level == 'sentence':\n",
    "                features = np.array([(feat - np.mean(feat))/np.std(feat) for feat in features.T])\n",
    "            elif self.level == 'word':\n",
    "                df.iloc[:, 3:] = [(getattr(df,field).values - getattr(df,field).values.mean())/\\\n",
    "                                  getattr(df,field).values.std() for field in fields[3:]]\n",
    "                \n",
    "        if self.level == 'sentence':\n",
    "            if self.scaling == 'raw':\n",
    "                df = pd.DataFrame(data=features, index=range(features.shape[0]), columns=[fields])\n",
    "            else:\n",
    "                df = pd.DataFrame(data=features.T, index=range(features.shape[1]), columns=[fields])\n",
    "                \n",
    "            if self.fillna == 'zeros':\n",
    "                df.iloc[:,:].fillna(0, inplace=True)\n",
    "            elif self.fillna == 'min':\n",
    "                for i, field in enumerate(fields):\n",
    "                    df.iloc[:,i].fillna(getattr(df, field).values.min(), inplace=True)\n",
    "            elif self.fillna == 'mean':\n",
    "                for i, field in enumerate(fields):\n",
    "                    df.iloc[:,i].fillna(getattr(df, field).values.mean(), inplace=True)\n",
    "           \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_inf(features):\n",
    "        pop_idx = 0\n",
    "        for idx, feat in enumerate(features):\n",
    "            if True in np.isneginf(feat) or True in np.isinf(feat):\n",
    "                features = np.delete(features, idx-pop_idx, axis=0)\n",
    "                pop_idx += 1\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e459ef3c-3f85-42f2-926a-79e13c6a1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(sbjs): \n",
    "    \"\"\"\n",
    "        Args: Data per sbj on sentence level for task 1\n",
    "        Purpose: Function is necessary to control for order effects (only relevant for Task 1 (NR))\n",
    "    \"\"\"\n",
    "    first_half, second_half = [], []\n",
    "    for sbj in sbjs:\n",
    "        first_half.append(sbj[:len(sbj)//2])\n",
    "        second_half.append(sbj[len(sbj)//2:])\n",
    "    return first_half, second_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63d5f635-f7fa-4d19-b031-2c6f43296edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess.ipynb \u001b[34mscripts\u001b[m\u001b[m          \u001b[34mtask1- SR\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "be3b2a75-dbfa-40b1-90d4-cac7256fceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"task1- SR/Matlab_files/resultsZAB_SR.mat\"\n",
    "\n",
    "# index of the array `data` is the number of sentence\n",
    "data = io.loadmat(file_name, squeeze_me=True, struct_as_record=False)['sentenceData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c1d15db9-73f1-458d-8a21-e3168856a8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['content', 'fixPositions', 'nFixations', 'meanPupilSize', 'rawEEG', 'rawET', 'FFD', 'FFD_pupilsize', 'FFD_t1', 'FFD_t2', 'FFD_a1', 'FFD_a2', 'FFD_b1', 'FFD_b2', 'FFD_g1', 'FFD_g2', 'FFD_t1_diff', 'FFD_t2_diff', 'FFD_a1_diff', 'FFD_a2_diff', 'FFD_b1_diff', 'FFD_b2_diff', 'FFD_g1_diff', 'FFD_g2_diff', 'TRT', 'TRT_pupilsize', 'TRT_t1', 'TRT_t2', 'TRT_a1', 'TRT_a2', 'TRT_b1', 'TRT_b2', 'TRT_g1', 'TRT_g2', 'TRT_t1_diff', 'TRT_t2_diff', 'TRT_a1_diff', 'TRT_a2_diff', 'TRT_b1_diff', 'TRT_b2_diff', 'TRT_g1_diff', 'TRT_g2_diff', 'GD', 'GD_pupilsize', 'GD_t1', 'GD_t2', 'GD_a1', 'GD_a2', 'GD_b1', 'GD_b2', 'GD_g1', 'GD_g2', 'GD_t1_diff', 'GD_t2_diff', 'GD_a1_diff', 'GD_a2_diff', 'GD_b1_diff', 'GD_b2_diff', 'GD_g1_diff', 'GD_g2_diff', 'GPT', 'GPT_pupilsize', 'GPT_t1', 'GPT_t2', 'GPT_a1', 'GPT_a2', 'GPT_b1', 'GPT_b2', 'GPT_g1', 'GPT_g2', 'GPT_t1_diff', 'GPT_t2_diff', 'GPT_a1_diff', 'GPT_a2_diff', 'GPT_b1_diff', 'GPT_b2_diff', 'GPT_g1_diff', 'GPT_g2_diff', 'SFD', 'SFD_pupilsize', 'SFD_t1', 'SFD_t2', 'SFD_a1', 'SFD_a2', 'SFD_b1', 'SFD_b2', 'SFD_g1', 'SFD_g2', 'SFD_t1_diff', 'SFD_t2_diff', 'SFD_a1_diff', 'SFD_a2_diff', 'SFD_b1_diff', 'SFD_b2_diff', 'SFD_g1_diff', 'SFD_g2_diff']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# get all field names for sentence data\n",
    "# print(data[0]._fieldnames)\n",
    "\n",
    "# # example: print sentence\n",
    "# print(data[1].content)\n",
    "\n",
    "# # example: get omission rate of first sentence\n",
    "# omission_rate = data[0].omissionRate\n",
    "# print(omission_rate)\n",
    "\n",
    "# # get word level data\n",
    "word_data = data[0].word\n",
    "\n",
    "# # get names of all word features\n",
    "# # index of the array `word_data` is the number of the word\n",
    "print(word_data[0]._fieldnames)\n",
    "\n",
    "# # example: get first word\n",
    "# print(word_data[0].content)\n",
    "\n",
    "# # example: get number of fixations of first word\n",
    "print(word_data[0].nFixations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "72cb9e9e-e833-4703-82aa-87b7f8ecaceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Presents a good case while failing to provide a reason for us to care beyond the very basic dictums of human decency.'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8f3017e4-fdc3-456c-b584-7f5cc3a5c681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EEG = [ 'mean_t1',#x\n",
    "'mean_t2',#x\n",
    "'mean_a1',#x\n",
    "'mean_a2',#x\n",
    "'mean_b1', #x\n",
    "'mean_b2', #x\n",
    "'mean_g1', #x\n",
    "'mean_g2'] #x\n",
    "\n",
    "for i in sorted(os.listdir('task1- SR/Matlab_files')):\n",
    "    if '.mat' in i:\n",
    "        \n",
    "        file_name = f'task1- SR/Matlab_files/{i}'\n",
    "        data = io.loadmat(file_name, squeeze_me=True, struct_as_record=False)['sentenceData']\n",
    "        patient = f'{i[7:10]}'\n",
    "        eeg = pd.DataFrame()\n",
    "        L = []\n",
    "\n",
    "        for j in range(len(data)):\n",
    "\n",
    "            arr =  data[j].mean_g2\n",
    "\n",
    "            if np.isnan(arr).all():\n",
    "\n",
    "                arr = [0] * 105\n",
    "\n",
    "            df = pd.DataFrame(arr).T\n",
    "\n",
    "            eeg = pd.concat([eeg, df], axis = 0)\n",
    "\n",
    "        eeg= eeg.reset_index()\n",
    "\n",
    "        for k in range(len(data)):\n",
    "            sent = data[k].content\n",
    "            L.append(sent)\n",
    "\n",
    "        sent_df = pd.DataFrame(L, columns = ['new_words'])\n",
    "\n",
    "        new_df = pd.concat([sent_df, eeg], axis = 1)\n",
    "        new_df.to_csv(f'eeg/mean_g2/{patient}_mean_g2_df.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "7aeff42b-7a3d-43e4-a6e9-a99a7522f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "count = 0\n",
    "for i in sorted(os.listdir('eeg')):\n",
    "    if '.csv' and 'ZPH' in i:\n",
    "        read = pd.read_csv(f'eeg/{i}')\n",
    "        name = i[4:11]\n",
    "        count =0\n",
    "        for j in range(3, len(read.columns)):\n",
    "            read = read.rename(columns={f'{read.columns[j]}': f'{name}_{read.columns[j]}'})\n",
    "            count +=1\n",
    "            \n",
    "        read.to_csv(f'{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "b3d1bcf9-e4f4-4cc5-a1ef-01674561d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in sorted(os.listdir('eeg')):\n",
    "    if'.csv' and 'ZPH' in i:\n",
    "        read= pd.read_csv(f'eeg/{i}')\n",
    "        df = pd.concat([df, read],axis=1)\n",
    "\n",
    "df.to_csv('ZPH_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "3654b45d-2c03-4e06-b47a-208ae83c73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv('label.csv')\n",
    "for i in sorted(os.listdir('preprocessed_eeg')):\n",
    "    if '.csv' in i:\n",
    "        read = pd.read_csv(f'preprocessed_eeg/{i}')\n",
    "        df = pd.concat([sentiment, read], axis = 1)\n",
    "        df.to_csv(f'{i}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e8697-9c5f-497a-81f0-7ffdddd418c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "read = pd.read_csv('preprocessed_eeg/ZAB_mean.csv')\n",
    "read.drop(['Unnamed: 0', 'Unnamed: 0.1', 'index'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bc502-9627-4506-b254-42edca0097f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "read.to_csv('ZAB_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e25db4-83f7-4395-a784-51446e6b80ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
